# Data Aggregation Scripts

This directory contains Python scripts used to process and aggregate the raw benchmark data generated by the [Criterion.rs](https://github.com/bheisler/criterion.rs) framework. These scripts are essential for transforming the JSON output from the performance experiments into structured CSV files suitable for analysis and visualisation.

## Overview

The benchmarking process for both the raw cryptographic primitives and the PQC-integrated OpenMLS operations produces a large number of data files. These scripts automate the tedious process of parsing these files, extracting the relevant performance metrics, enriching the data with metadata, and compiling it into a clean, unified format.

## Scripts

### `aggregate_primitives.py`

This script is responsible for processing the benchmark results from the `pqc-primitive-bench` experiment.

-   **Purpose**: To aggregate performance data for raw cryptographic operations (Key Generation, Signing, Verification).
-   **Input**: The root directory of the Criterion output for the primitives benchmark (e.g., `pqc-primitive-bench/target/criterion`).
-   **Output**: A single CSV file (`primitives_benchmark_results.csv`) containing the performance metrics for each signature scheme.

### `aggregate_results.py`

This script handles the more complex benchmark results from the `pqc-openmls` experiment.

-   **Purpose**: To aggregate performance data for high-level MLS protocol operations (e.g., Member Addition, Self-Update, Application Messaging).
-   **Input**: The root directory of the Criterion output for the OpenMLS benchmarks (e.g., `pqc-openmls/target/criterion`).
-   **Output**:
    1.  A **master CSV file** (`master_benchmark_results.csv`) containing all processed records.
    2.  A set of **focused CSV files**, where the data is split by operation type into a separate directory (e.g., `results/focused_ops/`). This is useful for targeted analysis of specific MLS operations.

## Requirements

-   Python 3.x
-   Pandas
-   tqdm

You can install the required Python packages using pip:

```sh
pip install pandas tqdm
```

## Workflow and Usage

The intended workflow is to first run the benchmarks using `cargo bench` and then use these scripts to process the output. The scripts should be run from the `data-scripts` directory.

### 1. Aggregating Primitive Benchmarks

After running the benchmarks in the `pqc-primitive-bench` directory, you can aggregate the results with the following command:

```sh
python aggregate_primitives.py \
    -i ../pqc-primitive-bench/target/criterion \
    -o ./results/primitives_benchmark_results.csv
```

### 2. Aggregating OpenMLS Benchmarks

Similarly, after running the benchmarks in the `pqc-openmls` directory, use this command to aggregate the results:

```sh
python aggregate_results.py \
    -i ../pqc-openmls/target/criterion \
    -o ./results/master_benchmark_results.csv \
    -d ./results/focused_ops
```

### Command-Line Arguments

Both scripts accept command-line arguments to specify the input and output paths:

-   `-i`, `--input-dir`: Specifies the path to the root directory of the Criterion.rs benchmark output.
-   `-o`, `--output-file`: Specifies the path to save the aggregated CSV output file.
-   `-d`, `--output-dir` (for `aggregate_results.py` only): Specifies a directory to save the focused, per-operation CSV files.

If you run the scripts without arguments, they will use the default paths specified within them.
