"""
aggregate_primitives.py

This script processes raw benchmark data for cryptographic primitives generated by
the Criterion.rs framework. It recursively scans a specified directory for
benchmark results, parses the relevant JSON files ('benchmark.json' for metadata
and 'estimates.json' for performance metrics), and aggregates this data into a
single, well-structured CSV file.

Usage:
    python aggregate_primitives.py -i <path_to_criterion_output> -o <output_csv_file>
"""
import os
import json
import re
import pandas as pd
from tqdm import tqdm
import logging
import argparse

# Configure basic logging to report informational messages and errors.
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')

# Maps signature scheme names to their cryptographic family and security level.
# This provides essential context for analysis. The keys must exactly match the
# names returned by the `name()` function in the Rust benchmark implementation.
METADATA_MAP = {
    "Ed25519": ("Classical", "N/A"),
    # NIST Level 1
    "ML-DSA-44": ("Lattice", "NIST Level 1"),
    "SPHINCS-SHA2-128f": ("Hash-Based", "NIST Level 1"),
    "SPHINCS-SHA2-128s": ("Hash-Based", "NIST Level 1"),
    "FALCON-512": ("Lattice", "NIST Level 1"),
    # NIST Level 3
    "ML-DSA-65": ("Lattice", "NIST Level 3"),
    "SPHINCS-SHA2-192f": ("Hash-Based", "NIST Level 3"),
    "SPHINCS-SHA2-192s": ("Hash-Based", "NIST Level 3"),
    # NIST Level 5
    "ML-DSA-87": ("Lattice", "NIST Level 5"),
    "SPHINCS-SHA2-256f": ("Hash-Based", "NIST Level 5"),
    "SPHINCS-SHA2-256s": ("Hash-Based", "NIST Level 5"),
    "FALCON-1024": ("Lattice", "NIST Level 5"),
}


def main(root_dir, output_csv):
    """
    Walks through benchmark directories, parses performance data, and generates a master CSV.

    Args:
        root_dir (str): The path to the root directory of the Criterion.rs output.
                        The script will search for subdirectories containing benchmark results.
        output_csv (str): The path where the final aggregated CSV file will be saved.
    """

    os.makedirs(os.path.dirname(output_csv), exist_ok=True)

    all_records = []

    # First, identify all valid benchmark directories to provide a progress bar.
    benchmark_dirs = []
    for root, dirs, files in os.walk(root_dir):
        # A valid directory contains a 'new' subdirectory with the expected JSON files.
        if 'new' in dirs:
            new_dir_path = os.path.join(root, 'new')
            if os.path.exists(os.path.join(new_dir_path, 'benchmark.json')) and \
               os.path.exists(os.path.join(new_dir_path, 'estimates.json')):
                benchmark_dirs.append(root)

    print(f"Found {len(benchmark_dirs)} benchmark result directories in '{os.path.abspath(root_dir)}'. Processing...")

    # Process each identified benchmark directory.
    for dir_path in tqdm(benchmark_dirs, desc="Aggregating Primitives"):
        try:
            benchmark_json_path = os.path.join(
                dir_path, 'new', 'benchmark.json')
            estimates_json_path = os.path.join(
                dir_path, 'new', 'estimates.json')

            with open(benchmark_json_path, 'r') as f:
                metadata = json.load(f)
            with open(estimates_json_path, 'r') as f:
                estimates = json.load(f)

            # --- Parse and Enrich Data ---

            # The 'group_id' from Criterion corresponds to the benchmarked operation.
            group_id = metadata.get('group_id', 'Unknown')
            # Remove the numeric prefix (e.g., "1. ") for a clean operation name.
            operation = re.sub(r'^\d+\.\s+', '', group_id).strip()

            # The 'value_str' from Criterion is the name of the signature scheme.
            sig_scheme = metadata.get('value_str', 'Unknown')

            # Add family and security level from the metadata map.
            family, security_level = METADATA_MAP.get(
                sig_scheme, ("Unknown", "Unknown"))

            # Extract performance estimates (mean and median).
            mean = estimates.get('mean', {})
            median = estimates.get('median', {})

            # Compile the data into a single record.
            record = {
                'Operation': operation,
                'Signature_Scheme': sig_scheme,
                'Algorithm_Family': family,
                'Security_Level': security_level,
                'Mean_Time_ns': mean.get('point_estimate'),
                'Mean_Lower_Bound_ns': mean.get('confidence_interval', {}).get('lower_bound'),
                'Mean_Upper_Bound_ns': mean.get('confidence_interval', {}).get('upper_bound'),
                'Median_Time_ns': median.get('point_estimate'),
                'Median_Lower_Bound_ns': median.get('confidence_interval', {}).get('lower_bound'),
                'Median_Upper_Bound_ns': median.get('confidence_interval', {}).get('upper_bound'),
                'Source_Path': os.path.relpath(estimates_json_path, root_dir)
            }
            all_records.append(record)

        except Exception as e:
            logging.error(f"Failed to process directory '{dir_path}': {e}")

    # --- Create and Save DataFrame ---

    if not all_records:
        print(
            f"No benchmark records were found. Please check the directory '{root_dir}'.")
        return

    df = pd.DataFrame(all_records)

    # Enforce a consistent and logical column order for the output CSV.
    column_order = [
        'Operation', 'Signature_Scheme', 'Algorithm_Family', 'Security_Level',
        'Mean_Time_ns', 'Mean_Lower_Bound_ns', 'Mean_Upper_Bound_ns',
        'Median_Time_ns', 'Median_Lower_Bound_ns', 'Median_Upper_Bound_ns', 'Source_Path'
    ]
    df = df[column_order]

    df.to_csv(output_csv, index=False)

    # --- Final Report ---
    print("\n" + "="*50)
    print("      Primitives Data Aggregation Complete!")
    print("="*50)
    print(f"Successfully processed {len(df)} benchmark records.")
    print(f"Primitives dataset saved to: '{os.path.abspath(output_csv)}'")
    print("\nFirst 5 rows of the dataset:")
    print(df.head().to_string())
    print("\nReady for analysis!")


if __name__ == '__main__':
    # Setup command-line argument parsing.
    parser = argparse.ArgumentParser(
        description="Aggregate raw cryptographic primitive benchmarks from Criterion.rs into a single CSV.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument(
        '-i', '--input-dir',
        default='../pqc-primitive-bench/target/criterion',
        help="Path to the root directory of the Criterion.rs benchmark output."
    )
    parser.add_argument(
        '-o', '--output-file',
        default='./results/primitives_benchmark_results.csv',
        help="Path to save the aggregated CSV output file."
    )

    args = parser.parse_args()
    main(root_dir=args.input_dir, output_csv=args.output_file)
